{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U sentence-transformers","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-1.1.1.tar.gz (81 kB)\n\u001b[K     |████████████████████████████████| 81 kB 713 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.5.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.59.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.8.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.19.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.24.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.5.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.95)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (0.6)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2021.3.17)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.25.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.15.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (7.2.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.1-py3-none-any.whl size=123336 sha256=c49cee307264a858a8707fdfa990aa48a3c481c087f0b4ded14578df8e39051f\n  Stored in directory: /root/.cache/pip/wheels/9d/f2/81/9a97074f4974b3ade9fee286b3ea9acba88e7c9282928ba187\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-1.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport math\nfrom sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nimport logging\nfrom datetime import datetime\nimport os\nimport gzip\nimport csv\nimport pickle \nimport pandas as pd\nimport numpy as np\nfrom random import shuffle\nimport nltk\nimport re\nimport itertools\nfrom sklearn.model_selection import KFold\nfrom nltk.tokenize import sent_tokenize\nfrom functools import reduce\nimport torch","metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#### Just some code to print debug information to stdout\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\n#### /print debug information to stdout\n\n#Check if dataset exsist. If not, download and extract  it\nsts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n\nif not os.path.exists(sts_dataset_path):\n    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/392k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04406c05270e42559fd6fa5ce331bdb7"}},"metadata":{}}]},{"cell_type":"code","source":"# Read the dataset\nmodel_name = 'allenai/scibert_scivocab_uncased'\ntrain_batch_size = 4\nnum_epochs = 4\nmodel_save_path = 'output/training_arxiv_continue_training-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Read the arxiv corpus dataset \nclass Corpus:\n    def __init__(self, path_to_pkl):\n        self.data = None\n        self.load_pickle(path_to_pkl)\n\n    def load_pickle(self, path_to_pkl):\n        with open(path_to_pkl, 'rb') as f:\n            self.data = pickle.load(f)\n\n    def get_data(self):\n        return self.data\n\n    def get_keys(self):\n        keys_list = []\n        for k in self.data.keys():\n            keys_list.append(k)\n        return keys_list\n\n    def get_values(self):\n        values_list = []\n        for k, v in self.data.items():\n            values_list.append(v)\n        return values_list\n\n    def get_paper_by_id(self, id):\n        return self.data[id]\n    \ndef pre_process(paragraph):\n    paragraph = re.sub(r'\\S*@\\S*\\s?', '', paragraph, flags=re.MULTILINE)  # remove email\n    paragraph = re.sub(r'http\\S+', '', paragraph, flags=re.MULTILINE)  # remove web addresses\n    paragraph = paragraph.replace(\"et al.\", \"\")\n    return paragraph\n\n\ndef get_sentences_from_paragraph(paragraph):\n    sentences = nltk.sent_tokenize(paragraph)\n    return sentences\n\ndef flatten_list(sen_list):\n    return list(itertools.chain.from_iterable(sen_list))\n\ndef create_tsv(m_list):\n    columnName = [\"id1\", \"id2\", \"sentence1\", \"sentence2\"]\n    half_count = len(m_list)/2\n    # Using Dictionary comprehension\n    myDict = {key: None for key in columnName}\n\n    count = 0\n    list1 = []\n    list2 = []\n\n    for line in range(len(m_list)):\n\n        if (count < half_count):\n            list1.append(m_list[line])\n            myDict[\"sentence1\"] = list1\n        else:\n            list2.append(m_list[line])\n            myDict[\"sentence2\"] = list2\n        count += 1\n    df = pd.DataFrame(myDict)\n    id = list(map(str, np.arange(df.shape[0])))\n    df[\"id1\"] = np.core.defchararray.add('sent1', id)\n    df[\"id2\"] = np.core.defchararray.add('sent2', id)\n   # df.to_csv(\"../arxiv_data/output.tsv\", sep=\"\\t\", index=False)\n\npath_to_pkl = \"../input/corpus/corpus_dict.pkl\"\ncorpus = Corpus(path_to_pkl)\nlist_keys = corpus.get_keys()\n\nmain_list =[]\nfor k in list_keys:\n    paper = corpus.get_paper_by_id(k)\n\n    para_paper = list(map(pre_process, paper))\n    sentences = [get_sentences_from_paragraph(p) for p in para_paper]\n    merged = flatten_list(sentences)\n    main_list.append(merged)\n\n\nflattened_main_list = flatten_list(main_list)\n\nshuffle(flattened_main_list)\nprint(flattened_main_list[:10])\ncreate_tsv(flattened_main_list)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['Fig.', 'However, [8] recently showed that the alternating minimization method in fact converges to the global minima of two low-rank estimation problems: matrix sensing with RIP matrices and matrix completion.', 'Traditionally, cancer survival curves have been estimated using Kaplan-Meier methods [5].', 'This shortcoming is typical for convergence proofs that use similar types of conditions (cf.', 'Let us now focus on binary label prediction, that is Y={±1}.', 'Moreover, the algorithm runs in time O(qnk2) and succeeds with high probability when number of samples q=Ω(mlog2m).', 'We vary sparsity of S∗ and rank of L∗ for RTD with a fixed tensor size.', 'If the iteration complexity of an oblivious optimization algorithm for smooth and convex finite sum problems equipped with a first-order and a coordinate-descent oracle is of the form of the l.h.s.', 'Assuming that all counts (Nk)k=1,…,K are strictly positive, the criterion', 'thanks to the symmetry of the matrix Δ⊤Fπ∗ (cf.']\n","output_type":"stream"}]},{"cell_type":"code","source":"#Create two list of sentences \npath_sentence_pairs = \"../input/sentence-pairs/arxiv_corpus_new.tsv\"\nunlabelled_df = pd.read_csv(path_sentence_pairs, sep='\\t', quoting=csv.QUOTE_NONE)\n\nsentence1 = unlabelled_df['sentence1'].tolist()\nsentence2 = unlabelled_df['sentence2'].tolist()\n\ns1 = np.array(sentence1)\ns2 = np.array(sentence2)\n\n#Splitting the unlabelled dataset into k folds for self training\ndef return_k_fold(un_texts):\n    #Splitting the unlabelled dataset into k folds for self training\n    kf = KFold(n_splits=5, shuffle=True)\n\n    X_fold = list()\n\n    for _, fold in kf.split(un_texts):\n        X_fold.append(un_texts[fold])\n\n    X_fold = np.array(X_fold, dtype=\"object\")\n    \n    return X_fold\n\nX_fold_s1 = return_k_fold(s1)\nX_fold_s2 = return_k_fold(s2)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_new_train_data(predictions, fold_n):\n    X_new_s1 = list()\n    y_new = list()\n    X_new_s2 = list()\n\n       \n    for i, x in enumerate(predictions):\n        if x[i] > 0.90 or x[i] < 0.10:\n            X_new_s1.append(X_fold_s1[fold_n][i])\n            X_new_s2.append(X_fold_s2[fold_n][i])\n            y_new.append(x[i])\n           \n    return np.array(X_new_s1), np.array(y_new), np.array(X_new_s2)\n\n\ndef join_shuffle(train_samples, X_new_s1, y_new, X_new_s2):\n    \n    for s1, s2, label in zip(X_new_s1, X_new_s2, y_new):\n        inp_new = InputExample(texts=[s1, s2], label=label)\n        train_samples.append(inp_new)\n    return train_samples \n","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load a pre-trained sentence transformer model\nmodel = SentenceTransformer(model_name)\nmodel.max_seq_length = 128","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Exception when trying to download http://sbert.net/models/allenai/scibert_scivocab_uncased.zip. Response 404\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1bfcd0891e4d26888d73f0d1a27a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5315330fbc64ecdb77b2e3cbc62ad4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f70ab13a1df4454bbf88d6940424beb"}},"metadata":{}}]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/annotations/annotations_arxiv_merged.tsv\", sep='\\t',quoting=csv.QUOTE_NONE)\ntest_labels = test_df.Classification.values\n\ntest = []\n\nfor s1, s2, sco in zip(test_df['sentence1'].tolist(), test_df['sentence2'].tolist(), test_labels.tolist()):\n    tst_example = InputExample(texts=[s1, s2], label=sco)\n    test.append(tst_example)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Convert the dataset to a DataLoader ready for training\nlogging.info(\"Read STSbenchmark train dataset\")\n\ntrain_samples = []\ndev_samples = []\ntest_samples = []\nwith gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n    for row in reader:\n        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\n        inp_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=score)\n\n        if row['split'] == 'dev':\n            dev_samples.append(inp_example)\n        elif row['split'] == 'test':\n            test_samples.append(inp_example)\n        else:\n            train_samples.append(inp_example)\n            ","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\ntrain_loss = losses.CosineSimilarityLoss(model=model)\nprint(type(train_dataloader))","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'torch.utils.data.dataloader.DataLoader'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Development set: Measure correlation between cosine score and gold labels\nlogging.info(\"Read STSbenchmark dev dataset\")\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Configure the training. We skip evaluation in this example\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\nlogging.info(\"Warmup-steps: {}\".format(warmup_steps))","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)","metadata":{"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e3f414f13b4b4b933631986b459291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1438 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5803ad552cdb4f578391ac5eee60a7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1438 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960b4e491d024b3d9021462812961c22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1438 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec3af2455e1444b8914c5be2cedb482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1438 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24dfbd7b8557477f92fa273475f7e5f1"}},"metadata":{}}]},{"cell_type":"code","source":"index = 0\nfor fold1, fold2 in zip(X_fold_s1, X_fold_s2):\n    embeddings1 = model.encode(fold1, convert_to_tensor=True)\n    embeddings2 = model.encode(fold2, convert_to_tensor=True)\n    \n    #Compute cosine-similarits\n    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n    \n    X_new_s1, y_new, X_new_s2 = get_new_train_data(cosine_scores, index)\n    \n    print(f\"{len(X_new_s1)} high-probability predictions added to training data.\")\n    \n    if(len(X_new_s1) != 0):\n        train_samples = join_shuffle(train_samples, X_new_s1, y_new, X_new_s2)\n    \n    t_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n    t_loss = losses.CosineSimilarityLoss(model=model)\n\n    model.fit(train_objectives=[(t_dataloader, t_loss)],\n          evaluator=evaluator,\n          epochs=2,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)\n    index+=1","metadata":{"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3088002b8b744f7983e269977a0b950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9aa176a54194f82a745d42c1e5b27eb"}},"metadata":{}},{"name":"stdout","text":"2588 high-probability predictions added to training data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da33c9335d6e4673858cffc1e86471b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2085 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c0563d5dedc4c11a7c2c33d77daeb2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2085 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b5a8c586c5346598d75dd4cfcd410a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1889070863564f62b80ad15299257bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba7050e518bf41328b3072967162f4d7"}},"metadata":{}},{"name":"stdout","text":"4306 high-probability predictions added to training data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe7bd2fa51f4b5a85f2d37b08066d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/3161 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb423a8cfe7447b8c7b9da1631c030d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/3161 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccb42d39918047f496e8bb5dc7d39d56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d630c16fd0ae4578aef85004f79be62b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db6917805d54f9aa7542272c7e8fd92"}},"metadata":{}},{"name":"stdout","text":"6024 high-probability predictions added to training data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94191f3b20a430db359b4ee4566ce81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/4667 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7af841b4c740edafaf81fba86a4b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/4667 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eccf1ed57b74b869e544d36f654fce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d218876e9be74da8b372e3a61e9d8adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1486593f66884c52ab17240197e87edf"}},"metadata":{}},{"name":"stdout","text":"7907 high-probability predictions added to training data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de5ba7a59a547c2933485a7403e1456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6644 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d7b3bf5fb544739796627a87a75f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6644 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d87a64e50eb74dec89a885ef30f1418b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8960cbd65d40b3842f8d39f0cb6efc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddfd77ae89744cd3bf1f73b492044019"}},"metadata":{}},{"name":"stdout","text":"9498 high-probability predictions added to training data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19fe41cebe049c89388a567da33107f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/9018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db4496f30794ac481cc6fefb54cc253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/9018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786acf81f64d4b75b9d2d8eadd98becc"}},"metadata":{}}]},{"cell_type":"code","source":"model = SentenceTransformer(model_save_path)\ntest_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test, name='arxiv-test')\ntest_evaluator(model, output_path=model_save_path)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.5399054589118731"},"metadata":{}}]},{"cell_type":"code","source":"#RUN PREDICTION ON TEST SENTENCES \n#Sentences are encoded by calling model.encode()\nemb1 = model.encode(\"Unsupervised training data for the sentence encoding models are drawn from a variety of web sources\")\nemb2 = model.encode(\"We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference(SNLI) CORPUS\")\n\nemb3 = model.encode(\"Unsupervised training data for the sentence encoding models are drawn from a variety of web sources\")\nemb4 = model.encode(\"Rather the network maintains a scalable linear efficiency across all layers, realizing the transformer full potential\")\n\n\ncos_sim_1 = util.pytorch_cos_sim(emb1, emb2)\ncos_sim_2 = util.pytorch_cos_sim(emb3, emb4)\nprint(\"Cosine-Similarity:\", cos_sim_1)\nprint(\"Cosine-Similarity:\", cos_sim_2)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ae80bb8fc34c7d9d296f34ffe87f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b01d64508bf49569dac17285bd0502c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ea60931c7774ba0bf15a34e3b3faf30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b678359290cc4a5b83ff8060c09a7a6b"}},"metadata":{}},{"name":"stdout","text":"Cosine-Similarity: tensor([[0.8941]])\nCosine-Similarity: tensor([[0.1726]])\n","output_type":"stream"}]},{"cell_type":"code","source":"dic_para = {'paragraph1': \n     'Multi-objective optimization aims at finding trade-off solutions to conflicting objectives. These constitute the Pareto optimal set. In the context of expensive-to-evaluate functions, it is impossible and often non-informative to look for the entire set. As an end-user would typically prefer a certain part of the objective space, we modify the Bayesian multi-objective optimization algorithm which uses Gaussian Processes to maximize the Expected Hypervolume Improvement, to focus the search in the preferred region. The acumulated effects of the Gaussian Processes and the targeting strategy lead to a particularly efficient convergence to the desired part of the Pareto set. To take advantage of parallel computing, a multi-point extension of the targeting criterion is proposed and analyzed.'\n, 'paragraph2':\n      'We consider the problem of constrained multi-objective (MO) blackbox optimization using expensive function evaluations, where the goal is to approximate the true Pareto set of solutions satisfying a set of constraints while minimizing the number of function evaluations. We propose a novel framework named Uncertainty-aware Search framework for Multi-Objective Optimization with Constraints (USeMOC) to efficiently select the sequence of inputs for evaluation to solve this problem. The selection method of UseMOC consists of solving a cheap constrained MO optimization problem via surrogate models of the true functions to identify the most promising candidates and picking the best candidate based on a measure of uncertainty. We applied this framework to optimize the design of a multi-output switched-capacitor voltage regulator via expensive simulations. Our experimental results show that UseMOC is able to achieve more than 90 % reduction in the number of simulations needed to uncover optimized circuits.'\n}\nsource_paragraph = dic_para['paragraph1']\ntarget_paragraph = dic_para['paragraph2']\nsource_sentences = sent_tokenize(source_paragraph)\ntarget_sentences = sent_tokenize(target_paragraph)\n\n#Compute embedding for both lists\nembeddings1 = model.encode(source_sentences, convert_to_tensor=True)\nembeddings2 = model.encode(target_sentences, convert_to_tensor=True)\n\n#Compute cosine-similarits\ncosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n\n\nmax_scores = []\nfor scores in cosine_scores:\n    max_elements, max_indices = torch.max(scores, dim=0)\n    max_index = max_indices.item()\n    max_scores.append(max_elements)\n\ndef count_average(lst):\n    return reduce(lambda a,b:a+b, lst) / len(lst)\navg = count_average(max_scores)\ndis_score = 1-avg\nprint(\"The two paragraphs have a similarity of {}\".format(avg))\nprint(\"The two paragraphs have a dissimilarity of {}\".format(dis_score))\n","metadata":{"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da7a51f2a084e589ed9bea846f555f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6907e3c2b1264e7d92f80c9ef68178e4"}},"metadata":{}},{"name":"stdout","text":"The two paragraphs have a similarity of 0.6267346739768982\nThe two paragraphs have a dissimilarity of 0.3732653260231018\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}