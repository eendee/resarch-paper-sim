An effective method of using Web based information for Relation Extraction
Yong Wai Keong, Stanley
Institute for Inforcomm Research
21 Heng Mui Keng Terrace,
Singapore 119613
wkyong@i2r.a-star.edu.sg
Su Jian
Institute for Inforcomm Research
21 Heng Mui Keng Terrace,
Singapore 119613
sujian@i2r.a-star.edu.sg
Abstract
We propose a method that incorporates
paraphrase information from the Web to
boost the performance of a supervised re-
lation extraction system. Contextual infor-
mation is extracted from the Web using a
semi-supervised process, and summarized
by skip-bigram overlap measures over the
entire extract. This allows the capture of lo-
cal contextual information as well as more
distant associations. We observe a statisti-
cally significant boost in relation extraction
performance.
We investigate two extensions, thematic
clustering and hypernym expansion. In tan-
dem with thematic clustering to reduce noise
in our paraphrase extraction, we attempt to
increase the coverage of our search for para-
phrases using hypernym expansion.
Evaluation of our method on the ACE 2004
corpus shows that it out-performs the base-
line SVM-based supervised learning algo-
rithm across almost all major ACE relation
types, by a margin of up to 31%.
1 Introduction and motivation
In this paper, we shall be primarily dealing with
the sort of relations defined in the NIST?s Auto-
matic Content Extraction program, specifically for
the Relation Detection and Characterization (RDC)
task (Doddington et al, 2004). These are links be-
tween two entities mentioned in the same sentence,
and further restrict our consideration to those rela-
tionships clearly supported by evidence in the scope
of the same document.
The ACE?s annotators mark all mentions of re-
lations where there is a direct syntactic connection
between the entities, i.e. when one entity mention
modifies another one, or when two entity mentions
are arguments of the same event. Relations between
entities that are implied in the text but which do not
satisfy either requirement are considered to be im-
plicit, and are marked only once.
Our work sits squarely in the realm of work on
regular IE done by (Zelenko et al, 2003; Zhou et
al., 2005; Chen et al, 2006). Here, the corpus of
interest is a well defined set of texts, such as news
articles, and we have to detect and classify all ap-
pearances of relations from a set of given relation
types in the documents. In line with assumptions
in the related work, we assert that the differences in
the markup for implicit and explicit relations does
not significantly affect our performance.
Supervised learning methods have proved to be
some of the most effective for regular IE. They do
however, need large volumes of expensively anno-
tated examples to perform robustly. As a result, even
the large ACE compilation has deficiencies in the
number of instances available for some of the rela-
tion types. (Zhang et al, 2005) reports an F-score
of 50% for categories of intermediate size and only
30% for the least common relation types. It appears
that there are hard limits on the performance of rela-
tion extraction systems as long as they have to rely
solely on information in the training set.
We were thus inspired to explore how one could
350
exploit theWeb, the largest raw text collection freely
available, for regular IE. In this paper, we detail
the ways one can fruitfully employ relation spe-
cific sentences retrieved from the Web with a semi-
supervised labeling approach. Most importantly we
show how the output from such an approach can be
combined with existing knowledge gleaned from su-
pervised learning to improve the performance of re-
lation extraction significantly.
2 Related Work and Differences
To our knowledge, there is no previous work that ex-
ploits the information from a large raw text corpus
like the Web to improve supervised relation extrac-
tion. In the spirit of the work done by (Shinyama
and Sekine, 2003; Bunescu and Mooney, 2007), we
are trying to collect clusters of paraphrases for given
relation mentions. Briefly, since the same relation
can be expressed in many ways, the information we
may learn about that relation in any single sentence
is very limited. The idea then is to alleviate this bias
by collecting many paraphrases of the same relation
instance into clusters when we train our system.
Shinyama generalizes the expressions using part-
of-speech information and dependency tree similar-
ity into generic templates. Bunescu?s work uses a
relation kernel on subsquences of words developed
in (Bunescu and Mooney, 2005). We observed that
both approaches suffer from low recall despite the
attempts to generalize the subsequences and tem-
plates probably because they rely on local context
only.
Based on our observation, we looked for a way to
use our clusters without losing non-local informa-
tion about the sentences. Bag-of-words or unigram
representations of our paraphrase clusters are easy
to compute, but information about word ordering is
lost. Hence, we settled on the use of a skip-bigram
representation of our relation clusters instead.
Skip-bigrams (Lin and Och, 2004) are pairs of
words in sentence order allowing for gaps in be-
tween. Longer gaps capture far-flung associations
between words, while short gaps of between 1 and 4
capture local structure. In using them, we do not re-
strict ourselves to context centered around the entity
mentions. Another advantage of using skip-bigrams
is that we can capture some extra-sentential infor-
mation since we are no longer restricted by the abil-
ity to generate dependency structures within a single
sentence as Shinyama is.
Using skip-bigrams, we can assess the similarity
of a particular new relation mention instance against
the relation clusters we collect in training. We can
then compute a likelihood that we combine with the
predictions of the supervised learning algorithm for
final classification.
Two possible extensions to the basic method
stated above were examined.
A central problem with the paraphrase collection
approach when applied to an open corpus is noise.
As pointed out by (Bunescu and Mooney, 2007),
even though the same entities co-occur in multiple
sentences, they are not necessarily linked by the
same relationship in all of them. The problem is
exacerbated when the open corpus we look at con-
tains documents from heterogenous domains. In-
deed, we cannot even assume that the predominant
relation that holds between two entities in the set of
sentences is the relation of interest to us.
One means of combating this is suggested by
(Bunescu and Mooney, 2007). They re-weight the
importance of word features in their model to re-
duce topic drift. We try a different solution based on
the thematic clustering of sentences. Sentences ex-
tracted from the raw corpus are mapped to a vector
space and partitioned into different clusters using the
Partitioning Around Medoids algorithm (Kaufmann
and Rousseeuw, 1987). Sentences in the clusters
closest to the original relation mention instance are
more likely to embody the same relationship. Hence
we retain such clusters, while discarding the rest. As
the relation we wish to recover may not be the pre-
dominant one, the cluster that is retained is also of-
ten not the largest one.
Another problem identified by Shinyama is that
the same entity may itself be referred to in differ-
ent ways. If the form used in the original relation
mention is uncommon, then few paraphrases will be
found. For instance, ??President Bush?? may be re-
ferred to as ??Dubya?? by a writer. Searching for
sentences online with the word ??Dubya?? and the
other entity participating in the relation is likely to
result in a collection heavily biased towards the orig-
inator of the nickname. Shinyama?s solution is to
use a limited form of co-reference resolution to re-
351
place these forms with a more general noun phrase.
As co-reference resolution is itself an unreliable pro-
cess, we suggest the use of hypernym substitution
instead.
In subsequent sections, we will outline the struc-
ture of our system, examine the experimental evi-
dence of its viability using the ACE program data,
and finish with a discussion of the extensions.
3 Overall structure
Our system is organized very naturally into two
main phases, a learning or training phase, followed
by a usage or testing phase. The learning phase is
subdivided into two parallel paths, reflecting the hy-
brid nature of our algorithm.
Fully supervised learning based on annotations
takes place in tandem with a semi-supervised algo-
rithm that captures the paraphrase clusters using en-
tity mention instances. We will combine the mod-
els from both the supervised learning and the semi-
supervised algorithm using a meta-classifier trained
a different subset of the data.
3.1 Learning Procedure
Our goal is to acquire as much contextual informa-
tion from the available annotations as possible via
our supervised learner and expand on that usingWeb
based information found by our semi-supervised al-
gorithm.
We constructed our fully supervised learner ac-
cording to the specifications for the system devel-
oped by (Zhou et al, 2005). It utilizes a feature
based framework with a Support Vector Machine
(SVM) classifier (Cortes and Vapnik, 1995). We
support the same set of features as Zhou, namely: lo-
cal word position features, entity type, base phrase
chunking features, dependency and parse tree fea-
tures as well as semantic information like coun-
try names and a list of trigger words. In our cur-
rent work, we use Michael Collins? (Collins, 2003)
parser for syntactic information.
Sentence boundary detection, chunking, and pars-
ing are done as preprocessing steps before we begin
our learning.
Given a sentence with the relation mention in-
stance, the semi-supervised method goes through the
following five stages:
1. From the list of entitites marked in the sen-
tence, generate all possible pairings as candi-
dates. We pick one of these candidates and pro-
ceed to the next step.
2. Gather hypernyms for each entity mention us-
ing Wordnet synsets and generate all possible
combinations of entity pairs from the two sets.
3. Find sentences from the Web that mention both
entities.
4. Cluster the sentences found using k-medoids,
filter out noise and retain only the cluster that
the original relation mention would be assigned
to.
5. Collate all clusters by relation type and gener-
ate a skip-bigram index for each relation type.
We will spend the rest of this section on the details
of each stage.
3.1.1 Extracting entity mentions and gathering
hypernyms
The process starts when we receive a relation
mention and the sentence it was found in, for in-
stance in the following sentence from the ACE 2004
corpus: ?As president of Sotheby?s, she often con-
ducted the biggest, the highest profile auctions.?
From the annotation, we find that the two
entities of interest are president and Sotheby?s
in the EMP-ORG Executive relation, which
we could represent as the predicate EMP ?
ORG Executive(em1, em2). Since our aim is to
find as many instances of semantically similar sen-
tences as possible, we want to lessen the negative
impact of quirky spelling or naming in a given sen-
tence. Hence we do a hypernym search in Wordnet
to find more general terms for our entities and create
list of similarly related entities (em1?, em2?) etc. If
the mention is a named entity, we do without the hy-
pernym expansion and use coreference information
(when available) to find the most common substring
amongst the mentions for the same entity.
In this example, it might result in the four pairs
show in Table 1.
352
Table 1: Examples of entity pairs after hypernym
expansion
President Sotheby?s
Chief Executive Sotheby?s
Decision maker Sotheby?s
leader Sotheby?s
Table 2: Examples of extracted text from Google
The 40 year old former president
travels incognitio to Sotheby?s
Brooks was named president of
Sotheby?s Inc.
Subsidiary President at Sotheby?s...
Bill Ruprecht, chief executive of Sotheby?s,
agreed that September 11 had been...
The Duke will also remain leader
of Sotheby?s Germany...
3.1.2 Web search
(Geleijnse and Korst, 2006) use Google as their
search engine for extracting surface patterns from
Web documents. We use the same procedure here
to find our paraphrases. For each pair of arguments,
we create a boolean query string em1 ? em2, and
submit it to Google. The query will find documents
where mentions of em1 are separated from em2 by
any number of words. We restrict the language to
English.
Google returns a two line extract from the docu-
ments that match our boolean query. The extracts
are generally those lines where the key query terms
are most densely collocated. These are parsed
and obviously nonsensical sentences are discarded
based on the occurrence of words from a list of stop
words. If a group of sentences are very similar,
we choose a single representative and discard the
rest. For every remaining sentence, we normalize
them by removing extraneous HTML tags. Some
examples of extracts found are listed in Table 2.
3.1.3 Cluster, filter and collate
In general, the collection of sentence extracts we
have at the end of the previous stage are likely to be
about a diverse range of topics. As we are only inter-
ested in the subset that is most compatible with the
thematic content of our original relation mention, we
will have to filter away unrelated extracts. For exam-
ple, the EMP-ORG Executive relation does not hold
in the sentence: ?The 40 year old former president
travels incognito to Sotheby?s?.
We make use of the K-medoids method by (Kauf-
mann and Rousseeuw, 1987). The terms from all
sentences are extracted and their frequency in each
sentence is computed. Each sentence is nowmapped
as a vector of frequencies in the space of the terms
that we observed. The resulting vectors are stored
as a large matrix. Picking a random partition of the
sentences as our starting point, we assign some sen-
tences to be the cluster centers, and iteratively refine
the clusters based on a distance minimization heuris-
tic.
Through some preliminary experiments, we find
that K-medoids based clustering with 5 classes pro-
duced the most consistent results. From a list of
excerpts, our algorithm culls the sentences that be-
longed to the 4 irrelevant clusters and produces
the excerpts which capture the original relationship
best.Since the quality of the partitions produced by
the algorithm is sensitive to the initial random start,
we do this process twice with different configura-
tions and take the union of the two clusters as our
final result.
The best excerpts are stored with accompanying
meta-data about the originating training relation in-
stance in what we call pseudo-documents. We group
the pseudo-documents in our database by the rela-
tion label that the instance pairs were given. Thus
we end up with several bags of pseudo-documents,
where each bag corresponds to a single relation type
of interest.
For computational efficiency, we generate an in-
verted hash-index for the pseudo-documents. Our
skip-bigrams act as the keys and the records are lists
of meta-data nodes. Each node records the sentence
that the bigram is observed in, the relation type of
that sentence, and the position of the bigram.
All we need now is a means of measuring the sim-
ilarity of a new relation mention instance with the
bags of pseudo-documents to assign a relation label
to it.
353
3.1.4 Skip-bigrams
As discussed in the introduction, instead of gen-
eralizing our bags of documents into patterns or re-
lation extraction kernels, we create skip-bigram in-
dices. There are several advantages in doing so.
Skip-bigrams are easy to compute relative to
the dependency trees or subsequence kernels used
in (Shinyama and Sekine, 2003) or (Bunescu and
Mooney, 2005). Moreover, we can tune the num-
ber of gaps allowed to capture long-distance word
dependency information, which is not done by the
other approaches because it is relatively more expen-
sive for them to do so, due to the combinatorial ex-
plosion. In addition, as compared to Bunescu?s sub-
sequence approach which needs 4 words to match,
bigrams are far less likely to be sparse in the docu-
ment space.
Since we relied upon skip-bigrams in our queries
to Google, it is only natural that we use it again in
assessing the similarity of two pseudo-documents.
Each pseudo-document is really an extractive sum-
mary of online articles about the same topic, with
the same entities. The degree of overlap between
two pseudo-documents is a good measure of their
thematic overlap.
Now that we have a metric, that still leaves the
question of our matching heuristic open. Do we au-
tomatically assign a test instance the relation label
of the sentence with the highest skip-bigram over-
lap? This naive approach is problematic. In gen-
eral, longer sentences will have more bigrams and
hence higher probability of overlapping with other
sentences. We could normalize the bigram overlap
score by the length of the sentences, but here we
leave the optimization to a machine learner.
Another possible heuristic is to pick the relation
label whose bag has the highest number of match-
ing bigrams with our test instance. Again, this will
be biased, but now towards bags with larger num-
bers of pseudo-documents. A last possibility is to
look at the total number of sentences that have bi-
gram matches, and weight the overlap score higher
for those with more sentence matches.
Therefore, instead of designing the heuristic ex-
plicitly, we use a validation set to observe the statis-
tical correlations of each of the three possible heuris-
tics we discussed above. We train an additional
model, using an SVM to choose the weights for each
heuristic automatically.
Accordingly, we do the following for each valida-
tion instance, V, and its pseudo-document P.
For each extracted sentence j in pseudo-
document P, we look up the database of pseudo-
documents from our training set, and compute the
skip-bigram similarity with every single sentence.
We have a skip-bigram similarity score for every sin-
gle sentence in the database with respect to V. The
scores are collated according to the relation classes.
For each relation class we generate three numbers,
TopS, Matching docs, and Total. Using the nota-
tion by (Lin and Och, 2004), we denote the skip-
bigram overlap between two sentences X and Y as
Skip2(X,Y ). For the ith relation, Ci is the set of all
pseudo-documents in our training set of that relation
type.
TopS = max
Y ?Ci,j?P
Skip2(Pj , Y ) (1)
Matching =
?
Y ?Ci
?
j?P
I[Skip2(Pj , Y ) > 0] (2)
Total =
?
Y ?Ci
?
j?P
Skip2(Pj , Y ) (3)
The three figures provide a summary of the best
sentence level match, and the overall relation level
overlap in terms of the number of sentences and
number of overlaps. As an illustration, we consider
the case where we have two sentences in our pseudo-
document P = P1, P2 and a relation RX . We com-
pute Skip2(P1, Y ) and Skip2(P2, Y ) by looking up
the skip-bigrams in the database for RX and aggre-
gating over sentences. Let?s assume that |RX| = 3
and only Skip2(P1, Y1) = 2, Skip2(P1, Y3) = 5,
Skip2(P2, Y3) = 4 are non-zero. Then TopS for in-
stance V is 5. Matching will be 2 since only Y1 and
Y3 have overlaps with elements of P . The Total is
simply 2 + 5 + 4 = 11.
3.2 Combining supervised with
semi-supervised models
After the preceding steps, we have a trained SVM
model based, and our skip-bigram index from the
semi-supervised procedure. In this section, we will
describe a method of combining these into a better
classifier.
354
The validation data we left aside earlier is sent
through our system with the relation labels removed.
Each entity pair in this validation set has a corre-
sponding pseudo-document and a file with numeri-
cal features for the SVM model.
An instance V is scored by Zhou?s SVM clas-
sifier, which assigns a relation tag, VST to it.
In parallel, the skip-bigram assessment results in
{TopS,Matching, Total} scores for each of the
relation classes. We treat the tag and numbers as fea-
tures for training another SVM, which we shall refer
to as SVMC . This is our final meta-classifier for
relation extraction on the tenth of the original data
set aside for testing. The meta-classifier may also be
used for completely new data.
4 Experimentation
We use the ACE corpus provided by the LDC from
2004 to train and evaluate our system. There are
674 annotated text documents and 9683 relation in-
stances in the set.
Starting with a single training set provided by
the user, we split that into three parts: the major-
ity (80%) is used for the learning phase, one tenth
is used for the validation during construction of the
combined model, and the remaining tenth is used for
testing. We ran a series of experiments, using five-
fold cross-validation.
Unlike typical cross-validation, the fifth that we
set aside is further sub-divided into two parts as we
stated before. Half is used when we construct the hy-
brid model merging supervised and semi-supervised
paths, and the remainder is used for the actual testing
and evaluation.
We used the 6 main relation types defined in the
annotation for the ACE dataset: ART, EMP-ORG,
GPE-AFF, OTHER-AFF, PER-SOC, and PHYS.We
computed three measures of the effectiveness, the
recall (R), precision (P) and F-1 score (F-1).
4.1 Comparison against the baseline
The first set of experiments we shall discuss com-
pares the overall system against our baseline. The
baseline system is implemented as a feature extrac-
tion module on top of a set of binary SVM classi-
fiers. A primary classifier is used to separate the
candidates without any relation of interest from the
rest. Secondary classifiers for each relation type are
then used to partition the positive candidates by vot-
ing. The performance of our baseline classifier when
tested on the ACE2003 dataset is statistically indis-
tinguishable from that reported by Zhou et al in
(Zhou et al, 2005).
Drilling down to the level of individual relation
classes as shown below, we note that the meta-
classifier performs better than the baseline on all but
one of the relations. This might be due to the inher-
ent ambiguity of the OTHER-AFF class.
Relation Ratio System R P F-1
ART
5.6 Hybrid 0.48 0.73 0.59
baseline 0.29 0.43 0.34
EMP-ORG
40.0 Hybrid 0.78 0.75 0.76
baseline 0.67 0.83 0.73
GPE-AFF
11.7 Hybrid 0.49 0.59 0.53
baseline 0.36 0.56 0.45
OTHER-AFF
3.4 Hybrid 0.14 0.18 0.16
baseline 0.18 0.59 0.28
PER-SOC
9.7 Hybrid 0.63 0.80 0.70
baseline 0.32 0.5 0.39
PHYS
29.4 Hybrid 0.75 0.59 0.66
baseline 0.40 0.64 0.49
The hybrid system has slightly lower precision
on the two largest relation classes, EMP-ORG and
PHYS, but higher recall, resulting in better F-scores
on both types. Finally, note that on the three inter-
mediate sized classes, ART, PER-SOC, and GPE-
AFF, the recall and precision were both higher. The
results suggest that the Web information does im-
prove recall substantially, but affects precision in
cases where there already is a substantial amount of
training data. It confirms our original assertion that
the hybrid approach works well for mid-sized rela-
tion classes, where the amount of training data is not
enough for the supervised system to perform opti-
mally.
We use the T-test to see if our improvements over
the baseline were significant at a 0.05 level. To
summarize, recall for the PHYS, PER-SOC, GPE-
AFF and EMP-ORG relations was improved signif-
icantly. The difference in precision is significant for
the PER-SOC, and OTHER-AFF classes, while F-1
score differences for the PER-SOC and PHYS rela-
tion classes at 0.00085, 0.032 respectively were both
significant.
355
4.2 Testing hypernym expansion and clustering
Subsequent experiments were aimed at quantifying
the contribution of hypernym expansion and the-
matic clustering to our hybrid system. We ran a
2 factorial experiment with four of our five folds,
where we take the hypernym expansion and cluster-
ing as treatments. Since we have more than one fea-
ture being tested, and we wish to observe the relative
contribution of each factor, we used an ANOVA test
instead of T-tests (Montgomery, 2005).
Our intuitive justification for hypernym expansion
was that recall would be boosted for relation types
where the name entities tend to be overly specific
in the training corpus. Place names, personal names
and the object names are obvious targets. Indeed, we
noted that the recall did increase in absolute terms
on average for the ART, PER-SOC and PHYS rela-
tion types (about 3% for each), but declined slightly
(about 0.5%) for the rest. Overall however, the size
of the effects was too small to be statistically sig-
nificant. This suggests that other methods of term
generalization may be needed to achieve a larger ef-
fect.
Next, we looked at the contribution of clustering.
Our initial experiments showed that k-medoids with
5 clusters was able to produce very precise clusters.
However, it would be at the expense of some of the
potential gain in recall form Web extracts.
Our experiments shows that clustering does in-
deed lower the potential recall. However, the hoped
for improvement in precision was observed only in
the PER-SOC (6%) and GPE-AFF (0.7%) relations.
This suggests that the effect of name entities having
multiple relations is concentrated in the classes of
named entities related to Persons and GPEs. Again,
the size of the effects was not statistically significant.
A more thorough investigation of clustering tech-
niques with different settings for k and different
algorithms will be needed before we can make
stronger statements.
5 Discussion and Conclusion
We have presented a hybrid approach to relation
extraction that incorporates Web based information
successfully to boost the performance of state-of-
the-art supervised feature based systems. Evaluation
on the ACE corpus shows that our skip-bigram based
relevance measures for finding the right paraphrase
in our Web extract database are very effective.
While our analysis shows that the addition of clus-
tering and hypernym expansion to the skip-bigram
based process is not statistically significant, we have
indications that the effect on recall and precision is
positive for certain relation classes.
In future work, we will examine improvements to
the clustering algorithm to reduce the impact on re-
call. We will look at alternative ways of attacking
the problem of name entity generalization and assess
the impact of methods like co-reference resolution in
the same ANOVA framework.
Acknowledgment
This research is supported by a Specific Tar-
geted Research Project (STREP) of the European
Union?s 6th Framework Programme within IST call
4, Bootstrapping Of Ontologies and Terminologies
STtragegic REsearch Project (BOOTStrep).
References
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In NIPS.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
576?583, Prague, Czech Republic, June. Association
for Computational Linguistics.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zheng-
Yu Niu. 2006. Relation extraction using label prop-
agation based semi-supervised learning. In ACL. The
Association for Computer Linguistics.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. volume 20, pages 273?297.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program. tasks, data and evaluation.
G. Geleijnse and J. Korst. 2006. Learning ef-
fective surface text patterns for information ex-
traction. In Proceedings of the EACL 2006
workshop on Adaptive Text Extraction and Min-
ing (ATEM 2006), pages 1 ? 8, Trento, Italy,
356
April. The Association for Computational Linguistics.
http://acl.ldc.upenn.edu/eacl2006/ws06 atem.pdf.
L. Kaufmann and P. J. Rousseeuw. 1987. Clustering
by means of medoids. In Y. Dodge, editor, Statistical
Data Analysis based on the L1 Norm, pages 405?416,
Amsterdam. Elsevier/North Holland.
Chin Y. Lin and Franz J. Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of Col-
ing 2004, pages 501?507, Geneva, Switzerland, Aug
FebruaryMarch?Aug FebruaryJuly. COLING.
Douglas C. Montgomery. 2005. Design and analysis of
experiments. John Wiley And Sons.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Kentaro Inui
and Ulf Hermjakob, editors, Proceedings of the Sec-
ond International Workshop on Paraphrasing, pages
65?71.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083?1106.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In IJCNLP, pages 378?
389.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In ACL. The Association for Computer Linguis-
tics.
357
