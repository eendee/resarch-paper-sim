Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 214?217,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
FBK-IRST: Semantic Relation Extraction using Cyc
Kateryna Tymoshenko and Claudio Giuliano
FBK-IRST
I-38050, Povo (TN), Italy
tymoshenko@fbk.eu, giuliano@fbk.eu
Abstract
We present an approach for semantic re-
lation extraction between nominals that
combines semantic information with shal-
low syntactic processing. We propose to
use the ResearchCyc knowledge base as
a source of semantic information about
nominals. Each source of information
is represented by a specific kernel func-
tion. The experiments were carried out
using support vector machines as a clas-
sifier. The system achieves an overall F
1
of 77.62% on the ?Multi-Way Classifica-
tion of Semantic Relations Between Pairs
of Nominals? task at SemEval-2010.
1 Introduction
The SemEval-2010 Task 8 ?Multi-Way Classifi-
cation of Semantic Relations Between Pairs of
Nominals? consists in identifying which seman-
tic relation holds between two nominals in a sen-
tence (Hendrickx et al, 2010). The set of rela-
tions is composed of nine mutually exclusive se-
mantic relations and the Other relation. Specifi-
cally, the task requires to return the most informa-
tive relation between the specified pair of nomi-
nals e
1
and e
2
taking into account their order. An-
notation guidelines show that semantic knowledge
about e
1
and e
2
plays a very important role in dis-
tinguishing among different relations. For exam-
ple, relations Cause-Effect and Product-Producer
are closely related. One of the restrictions which
might help to distinguish between them is that
products must be concrete physical entities, while
effects must not.
Recently, there has emerged a large number of
freely available large-scale knowledge bases. The
ground idea of our research is to use them as
source of semantic information. Among such re-
sources there are DBpedia,
1
YAGO,
2
and Open-
Cyc.
3
On the one hand, DBpedia and YAGO have
been automatically extracted from Wikipedia.
They have a good coverage of named entities, but
their coverage of common nouns is poorer. They
seem to be more suitable for relation extraction be-
tween named entities. On the other hand, Cyc is
a manually designed knowledge base, which de-
scribes actions and entities both in common life
and in specific domains (Lenat, 1995). Cyc has
a good coverage of common nouns, making it in-
teresting for our task. The full version of Cyc is
freely available to the research community as Re-
searchCyc.
4
We approached the task using the system intro-
duced by Giuliano et al (2007) as a basis. They
exploited two information sources: the whole sen-
tence where the relation appears, and WordNet
synonymy and hyperonymy information. In this
paper, we (i) investigate usage of Cyc as a source
of semantic knowledge and (ii) linguistic infor-
mation, which give useful clues to semantic re-
lation extraction. From Cyc, we obtain informa-
tion about super-classes (in the Cyc terminology
generalizations) of the classes which correspond
to nominals in a sentence. The sentence itself
provides linguistic information, such as local con-
texts of entities, bag of verbs and distance between
nominals in the context.
The different sources of information are rep-
resented by kernel functions. The final system
is based on four kernels (i.e., local context ker-
nel, distance kernel, verbs kernel and generaliza-
tion kernel). The experiments were carried out us-
ing support vector machines (Vapnik, 1998) as a
classifier. The system achieves an overall F
1
of
1
http://dbpedia.org/
2
http://www.mpi-inf.mpg.de/yago-naga/
yago/
3
http://www.cyc.com/opencyc
4
http://research.cyc.com/
214
77.62%.
2 Kernel Methods for Relation
Extraction
In order to implement the approach based on shal-
low syntactic and semantic information, we em-
ployed a linear combination of kernels, using the
support vector machines as a classifier. We de-
veloped two types of basic kernels: syntactic and
semantic kernels. They were combined by exploit-
ing the closure properties of kernels. We define the
composite kernelK
C
(x
1
, x
2
) as follows.
n
?
i=1
K
i
(x
1
, x
2
)
?
K
i
(x
1
, x
1
)K
i
(x
2
, x
2
)
. (1)
Each basic kernelK
i
is normalized.
All the basic kernels are explicitly calculated as
follows
K
i
(x
1
, x
2
) = ??(x
1
), ?(x
2
)? , (2)
where ?(?) is the embedding vector. The resulting
feature space has high dimensionality. However,
Equation 2 can be efficiently computed explicitly
because the representations of input are extremely
sparse.
2.1 Local context kernel
Local context is represented by terms, lemmata,
PoS tags, and orthographic features extracted
from a window around the nominals considering
the token order. Formally, given a relation ex-
ample R, we represent a local context LC =
t
?w
, ..., t
?1
, t
0
, t
+1
, ..., t
+w
as a row vector
?
LC
(R) = (tf
1
(LC), tf
2
(LC), ..., tf
m
(LC) ) ? {0, 1}
m
,
(3)
where tf
i
is a feature function which returns 1
if the feature is active in the specified position
of LC; 0 otherwise. The local context kernel
K
LC
(R
1
, R
2
) is defined as
K
LC e1
(R
1
, R
2
) +K
LC e2
(R
1
, R
2
), (4)
where K
LC e1
and K
LC e2
are defined by substi-
tuting the embedding of the local contexts of e
1
and e
2
into Equation 2, respectively.
2.2 Verb kernel
The verb kernel operates on the verbs present in
the sentence,
5
representing it as a bag-of-verbs.
5
On average there are 2.65 verbs per sentence
More formally, given a relation example R, we
represent the verbs from it as a row vector
?
V
(R) = (vf(v
1
, R), ..., vf(v
l
, R)) ? {0, 1}
l
, (5)
where the binary function vf(v
i
, R) shows if a
particular verb is used in R. By substituting
?
V
(R) into Equation 2 we obtain the bag-of-verbs
kernelK
V
.
2.3 Distance kernel
Given a relation example R(e
1
, e
2
), we repre-
sent the distance between the nominals as a one-
dimensional vector
?
D
(R) =
1
dist(e
1
, e
2
)
? <
1
, (6)
where dist(e
1
, e
2
) is number of tokens between
the nominals e
1
and e
2
in a sentence. By substitut-
ing ?
D
(R) into Equation 2 we obtain the distance
kernelK
D
.
2.4 Cyc-based kernel
Cyc is a comprehensive, manually-build knowl-
edge base developed since 1984 by CycCorp. Ac-
cording to Lenat (1995) it can be considered as
an expert system with domain spanning all ev-
eryday actions and entities, like Fish live in wa-
ter. The open-source version of Cyc named Open-
Cyc, which contains the full Cyc ontology and re-
stricted number of assertions, is freely available
on the web. Also the full power of Cyc has been
made available to the research community via Re-
searchCyc. Cyc knowledge base contains more
than 500,000 concepts and more than 5 million as-
sertions about them. They may refer both to com-
mon human knowledge like food or drinks and to
specialized knowledge in domains like physics or
chemistry. The knowledge base has been formu-
lated using CycL language. A Cyc constant repre-
sents a thing or a concept in the world. It may be
an individual, e.g. BarackObama, or a collection,
e.g. Gun, Screaming.
2.4.1 Generalization kernel
Given a nominal e, we map it to a set of Cyc
constants EC = {c
i
}, using the Cyc function
denotation-mapper. Nominals in Cyc usually de-
note constants-collections. Notice that we do not
performword sense disambiguation. For each c
i
?
EC, we query Cyc for collections which general-
ize it. In Cyc collection X generalizes collection
215
Y if each element of Y is also an element of col-
lectionX . For instance, collection Gun is general-
ized by Weapon, ConventionalWeapon, Mechani-
calDevice and others.
The semantic kernel incorporates the data from
Cyc described above. More formally, given a rela-
tion example R each nominal e is represented as
?
EC
(R) = (fc(c
1
, e), ..., fc(c
k
, e)) ? {0, 1}
k
, (7)
where the binary function fc(c
i
, e) shows if a par-
ticular Cyc collection c
i
is a generalization of e.
The bag-of-generalizations kernel
K
genls
(R
1
, R
2
) is defined as
K
genls e1
(R
1
, R
2
) +K
genls e2
(R
1
, R
2
) , (8)
whereK
genls e1
andK
genls e2
are defined by sub-
stituting the embedding of generalizations e
1
and
e
2
into Equation 2 respectively.
3 Experimental setup and Results
Sentences have been tokenized, lemmatized and
PoS tagged with TextPro.
6
Information for gener-
alization kernel has been obtained from Research-
Cyc. All the experiments were performed using
jSRE customized to embed our kernels.
7
jSRE
uses the SVM package LIBSVM (Chang and Lin,
2001). The task is casted as multi-class classifica-
tion problem with 19 classes (2 classes for each
relation to encode the directionality and 1 class
to encode Other). The multiple classification task
is handled with One-Against-One technique. The
SVM parameters have been set as follows. The
cost-factor W
i
for a given class i is set to be the
ratio between the number of negative and positive
examples. We used two values of regularization
parameter C: (i) C
def
=
1
?
K(x,x)
where x are
all examples from the training set, (ii) optimized
C
grid
value obtained by brute-force grid search
method. The default value is used for the other
parameters.
Table 1 shows the performance of different ker-
nel combinations, trained on 8000 training exam-
ples, on the test set. The system achieves the
best overall macro-average F
1
of 77.62% using
K
LC
+K
V
+K
D
+K
genls
. Figure 1 shows the
learning curves on the test set. Our experimen-
tal study has shown that the size of the training
6
http://textpro.fbk.eu/
7
jSRE is a Java tool for relation extraction avail-
able at http://tcc.itc.it/research/textec/
tools-resources/jsre.html.
1000 2000 4000 80000.40
0.450.50
0.550.60
0.650.70
0.750.80
0.850.90
Cause-EffectComponent-Whole Content-ContainerEntity-Destination Entity-Origin Instrument-Agency Member-CollectionMessage-TopicProduct-ProducerAll
Number of training examples
F1
Figure 1: Learning curves on the test set per rela-
tion
Kernels P R F
1
K
LC
+K
V
+K
D
+K
genls
74.98 80.69 77.62
K
LC
+K
V
+K
D
+K
genls
* 78.51 76.03 77.11
K
LC
+K
D
+K
genls
* 78.14 75.93 76.91
K
LC
+K
genls
* 78.19 75.70 76.81
K
LC
+K
D
+K
genls
72.98 80.28 76.39
K
LC
+K
genls
73.05 79.98 76.28
Table 1: Performance on the test set. Combina-
tions marked with * were run with C
grid
, others
with C
def
.
set influences the performance of the system. We
observe that when the system is trained on 8000
examples the overall F
1
increases for 14.01% as
compared to the case of 1000 examples.
4 Discussion and error analysis
The experiments have shown thatK
LC
is the core
kernel of our approach. It has good performance
on its own. For instance, it achieves precision of
66.16%, recall 72.67% and F
1
of 69.13% evalu-
ated using 10-fold cross-validation on the training
set.
Relation K
LC
K
LC
+K
genls
?F
1
Cause-Effect 74.29 76.41 2.12
Component-Whole 61.24 66.13 4.89
Content-Container 76.36 79.12 2.76
Entity-Destination 82.85 83.95 1.10
Entity-Origin 72.09 74.13 2.04
Instrument-Agency 57.71 65.51 7.80
Member-Collection 81.30 83.40 2.10
Message-Topic 60.41 69.09 8.68
Product-Producer 55.95 63.52 7.57
Table 2: The contribution of Cyc evaluated on the
training set.
216
Generalization kernel combined with local con-
text kernel gives precision of 70.38%, recall of
76.96%, and F
1
73.47% with the same exper-
imental setting. The increase of F
1
per re-
lation is shown in the Table 2 in the col-
umn ?F
1
. The largest F
1
increase is ob-
served for Instrument-Agency (+7.80%),Message-
Topic (+8.68%) and Product-Producer (+7.57%).
K
genls
reduces the number of misclassifications
between the two directions of the same rela-
tion, like Product-Producer(artist,design). It
also captures the differences among relations,
specified in the annotation guidelines. For in-
stance, the system based only on K
LC
misclass-
fied ?The<e1>species</e1>makes a squelching
<e2>noise</e2>? as Product-Producer(e2,e1).
Generalizations for <e2>noise</e2> provided
by Cyc include Event, MovementEvent, Sound.
According to the annotation guidelines a product
must not be an event. A system based on the com-
bination of K
LC
and K
genls
correctly labels this
example as Cause-Effect(e1,e2).
K
genls
improves the performance in general.
However, in some cases using Cyc as a source of
semantic information is a source of errors. Firstly,
sometimes the set of constants for a given nom-
inal is empty (e.g., disassembler, babel) or does
not include the correct one (noun surge is mapped
to the constant IncreaseEvent). In other cases,
an ambiguous nominal is mapped to many con-
stants at once. For instance, notes is mapped
to a set of constants, which includes Musical-
Note, Note-Document and InformationRecording-
Process. Word sense disambiguation should help
to solve this problem. Other knowledge bases like
DBpedia and FreeBase
8
can be used to overcome
the problem of lack of coverage.
Bag-of-word kernel with all words from the
sentence did not impact the final result.
9
However,
the information about verbs present in the sentence
represented by K
V
helped to improve the perfor-
mance. A preliminary error analysis shows that a
deeper syntactic analysis could help to further im-
prove the performance.
For comparison purposes, we also exploited
WordNet information by means of the supersense
kernel K
SS
(Giuliano et al, 2007). In all exper-
iments, K
SS
was outperformed by K
genls
. For
instance, K
LC
+ K
SS
gives overall F
1
measure
8
http://www.freebase.com/
9
This kernel has been evaluated only on the training data.
of 70.29% with the same experimental setting as
described in the beginning of this section.
5 Conclusion
The paper describes a system for semantic rela-
tions extraction, based on the usage of semantic
information provided by ResearchCyc and shal-
low syntactic features. The experiments have
shown that the external knowledge, encoded as
super-class information from ResearchCyc with-
out any word sense disambiguation, significantly
contributes to improve overall performance of the
system. The problem of the lack of coverage may
be overcome by the usage of other large-scale
knowledge bases, such as DBpedia. For future
work, we will try to use the Cyc inference en-
gine to obtain implicit information about nominals
in addition to the information about their super-
classes and perform word sense disambiguation.
Acknowledgments
The research leading to these results has received funding
from the ITCH project (http://itch.fbk.eu), spon-
sored by the Italian Ministry of University and Research and
by the Autonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint Research
Project under Future Internet - Internet of Content program
of the Information Technology Center, Fondazione Bruno
Kessler.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Claudio Giuliano, Alberto Lavelli, Daniele Pighin, and
Lorenza Romano. 2007. Fbk-irst: Kernel methods
for semantic relation extraction. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid
?
O S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2010. Semeval-2010 task 8: Multi-way classification of
semantic relations between pairs of nominals. In Proceed-
ings of the 5th SIGLEXWorkshop on Semantic Evaluation,
Uppsala, Sweden.
Douglas B. Lenat. 1995. CYC: A large-scale investment in
knowledge infrastructure. Communications of the ACM,
38(11):33?38.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
217
