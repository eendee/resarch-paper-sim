# -*- coding: utf-8 -*-
"""SyntacticFeatures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VO7XpqrFUly90pdqSY7JZkzWga3hx0wm
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
from nltk.corpus import wordnet
import numpy as np
np.random.seed(2018)
import pandas as pd
stemmer = SnowballStemmer('english')



"""JACCARD SIMMILARITY"""

def preprocess_no_stem(text):
    result = []
    
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(token)
            # tagged = nltk.pos_tag(result)
    return prep_no_stem( nltk.pos_tag(result) )

def prep_no_stem( ttaagg ):
  tags = []
  for i in ttaagg:
    if i[1].startswith(('N', 'V')):
      # tags.append(stemmer.stem(i[0]))
      tags.append(WordNetLemmatizer().lemmatize(i[0], pos='v'))
      # tags.append(i[0])
  return tags

def preprocess_stem(text):
    result = []
    
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(token)
            # tagged = nltk.pos_tag(result)
    return prep_stem( nltk.pos_tag(result) )

def prep_stem( ttaagg ):
  tags = []
  for i in ttaagg:
    if i[1].startswith(('N', 'V')):
      # tags.append(stemmer.stem(i[0]))
      tags.append(stemmer.stem(WordNetLemmatizer().lemmatize(i[0], pos='v')))
      # tags.append(i[0])
  return tags

def ssyn(paragraph):
  A2 = preprocess_no_stem(paragraph)
  synonyms = []
  for i in set(A2):
    for syn in wordnet.synsets(i):
      for l in syn.lemmas():
        synonyms.append(stemmer.stem(l.name()))
  # print(set(synonyms))
  return (set(synonyms))
# print(set(synonyms))

def jaccard_similarity(source_par, target_par):
    A = ssyn(source_par)
    B = ssyn(target_par)
    s1 = A
    s2 = B
    C = preprocess_stem(source_par)
    D = preprocess_stem(target_par)
    s3 = set(C)
    s4 = set(D)
    # s1 = set(A)
    # s2 = set(B)
    return float(len(s1.intersection(s4)) / len(s3.union(s4)))


def prep_ttr( ttaagg ):
  tags = []
  for i in ttaagg:
    if i[1].startswith(('N', 'V')):
      tags.append(stemmer.stem(i[0]))
      # tags.append(i[0])
  return tags


def preprocess_ttr(text):
    result = []
    
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(token)
            # tagged = nltk.pos_tag(result)
    return prep_ttr( nltk.pos_tag(result) )


def ttr_calc(paragraph):
  tokens = preprocess_ttr(paragraph)
  types = nltk.Counter(tokens)
  try:
    TTR = (len(types)/len(tokens))*100
  except ZeroDivisionError:
    TTR = 0
  return TTR


def ttr(source_par, target_par):
  source_ttr = ttr_calc(source_par)
  target_ttr = ttr_calc(target_par)

  source_ttr_bin = get_ttr_bins(source_ttr)
  target_ttr_bin = get_ttr_bins(target_ttr)

  sent = ""
  if source_ttr_bin != target_ttr_bin:
    sent = "The source paragraph has a relatively " + source_ttr_bin + " vocabulary richness while that of the target is " +  target_ttr_bin
  else:
    sent = "Both the source and target paragraphs have " + target_ttr_bin + " vocabulary richness"

  return sent


def get_ttr_bins(ttr_score):
    if ttr_score >= 80:
        return "High"
    elif ttr_score >= 60 and ttr_score < 80:
        return "Average"
    else:
        return "Low"

#ttr(unseen_document, unseen_document3)