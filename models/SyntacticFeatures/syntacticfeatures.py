# -*- coding: utf-8 -*-
"""SyntacticFeatures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VO7XpqrFUly90pdqSY7JZkzWga3hx0wm
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
from nltk.corpus import wordnet
import numpy as np
np.random.seed(2018)
import pandas as pd
stemmer = SnowballStemmer('english')

unseen_document = "The recommendation problem can be defined as estimating the response of a user for new items, based on historical information stored in the system, \
and suggesting to this user novel and original items for which the predicted response is high. User item responses can be numerical values known as ratings (e.g., 1–5 stars),\
 ordinal values (e.g., strongly agree, agree, neutral, disagree, strongly disagree) representing the possible levels of user appreciation, or binary values \
 (e.g., like/dislike or interested/not interested). Moreover, user responses can be obtained explicitly, forinstance, through ratings/reviews entered by users in the system, or implicitly, from purchase history or access patterns [39, 70].\
  For the purpose of simplicity, from this point on, we will call rating any type of user-item response."

unseen_document1 = 'An artificial neural network has a more complex structure than that of a \
perceptron model. The additional complexities may arise in a number of ways : \
1. The network may contain several intermediary layers between its input \
and output layers. Such intermediary layers are called hidden layers \
and the nodes embedded in these layers are called hidden nodes. The \
resulting structure is known as a multilayer neural network (see Figure \
5.17). In a feed-forward neural network, the nodes in one layer \
.Example of a multilayfeere d{onrvaarrdti ficianle uranl etwor(kA NN). \
are connected only to the nodes in the next layer. The perceptron is a \
single-layer, feed-forward neural network because it has only one layer \
of nodes-the output layer that performs complex mathematical operations. \
fn a recurrent neural network, the links may connect nodes \
within the same layer or nodes from one layer to the previous layers. \
2. The network may use types of activation functions other than the sign \
function. Examples of other activation functions include linear, sigmoid \
(logistic), and hyperbolic tangent functions, as shown in Figure 5.18. \
These activation functions allow the hidden and output nodes to produce \
output values that are nonlinear in their input parameters.'

unseen_document2 = 'There are different types of neural networks and the differences between them lies in their work principles, the scheme of actions, and the application areas. \
Convolutional neural networks (CNN) are mostly used for image recognition, and rarely for audio recognition. It is mostly applied to images because there is no need to check all the pixels one by one. \
CNN checks an image by blocks, starting from the left upper corner and moving further pixel by pixel up to a successful completion. Then the result of every verification is passed through a convolutional layer, \
where data elements have connections while others don’t. Based on this data, the system can produce the result of the verifications and can conclude what is in the picture'

unseen_document3 = 'Generally speaking, the reason people could be interested in using a recommender system is that they have so many items to choose from \
in a limited period of time—that they cannot evaluate all the possible options. A recommender should be able to select and \
filter all this information to the user. Nowadays, the most successful recommender systems have been built for entertainment content domains, \
such as: movies, music, or books.'

"""JACCARD SIMMILARITY"""

def preprocess_no_stem(text):
    result = []
    
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(token)
            # tagged = nltk.pos_tag(result)
    return prep_no_stem( nltk.pos_tag(result) )

def prep_no_stem( ttaagg ):
  tags = []
  for i in ttaagg:
    if i[1].startswith(('N', 'V')):
      # tags.append(stemmer.stem(i[0]))
      tags.append(WordNetLemmatizer().lemmatize(i[0], pos='v'))
      # tags.append(i[0])
  return tags

def preprocess_stem(text):
    result = []
    
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(token)
            # tagged = nltk.pos_tag(result)
    return prep_stem( nltk.pos_tag(result) )

def prep_stem( ttaagg ):
  tags = []
  for i in ttaagg:
    if i[1].startswith(('N', 'V')):
      # tags.append(stemmer.stem(i[0]))
      tags.append(stemmer.stem(WordNetLemmatizer().lemmatize(i[0], pos='v')))
      # tags.append(i[0])
  return tags

def ssyn(paragraph):
  A2 = preprocess_no_stem(paragraph)
  synonyms = []
  for i in set(A2):
    for syn in wordnet.synsets(i):
      for l in syn.lemmas():
        synonyms.append(stemmer.stem(l.name()))
  # print(set(synonyms))
  return (set(synonyms))
# print(set(synonyms))

def jaccard_similarity(source_par, target_par):
    A = ssyn(source_par)
    B = ssyn(target_par)
    s1 = A
    s2 = B
    C = preprocess_stem(source_par)
    D = preprocess_stem(target_par)
    s3 = set(C)
    s4 = set(D)
    # s1 = set(A)
    # s2 = set(B)
    return float(len(s1.intersection(s4)) / len(s3.union(s4)))

def jac_exp(source_par, target_par):
    A = ssyn(source_par)
    B = ssyn(target_par)
    s1 = A
    s2 = B
    C = preprocess_stem(source_par)
    D = preprocess_stem(target_par)
    s3 = set(C)
    s4 = set(D)
    inter = s1.intersection(s4)
    # s1 = set(A)
    # s2 = set(B)
    return "Both paragraphs share tokens " +  str(len(s1.intersection(s4))) + " which are " + str(inter) + " out of a total of " + str(len(s3.union(s4))) + " tokens in both paragraphs"

jaccard_similarity(unseen_document, unseen_document)

"""SENTENCE COMPLEXITY"""

# def ttr(source_par, target_par):
#   A = preprocess(source_par)
#   B = preprocess(target_par)
#   types_A = nltk.Counter(A)
#   types_B = nltk.Counter(B)
#   TTR_A = (len(types_A)/len(A))*100
#   TTR_B = (len(types_B)/len(B))*100
#   if max(TTR_A, TTR_B) - min(TTR_A, TTR_B) < 15:
#     print("Both paragraphs share similar Lexical Richness")
#     print("This is Lexical Richness Score for Source Paragraph is " + str(TTR_A) + " while that of Target Paragraph is " + str(TTR_B))
#     print((min(TTR_A, TTR_B)/max(TTR_A, TTR_B))*100)
#   else:
#     print("Both paragraphs have significantly dissimilar Lexical Richness")
#     print("This is Lexical Richness Score for Source Paragraph is " + str(TTR_A) + " while that of Target Paragraph is " + str(TTR_B))
#     print((min(TTR_A, TTR_B)/max(TTR_A, TTR_B))*100)

def prep_ttr( ttaagg ):
  tags = []
  for i in ttaagg:
    if i[1].startswith(('N', 'V')):
      tags.append(stemmer.stem(i[0]))
      # tags.append(i[0])
  return tags

# def lemmatize_stemming(text):
#     return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess_ttr(text):
    result = []
    
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(token)
            # tagged = nltk.pos_tag(result)
    return prep_ttr( nltk.pos_tag(result) )

def ttr_calc(paragraph):
  A = preprocess_ttr(paragraph)
  types = nltk.Counter(A)
  try:
    TTR = (len(types)/len(A))*100
  except ZeroDivisionError:
    TTR = 0
  # TTR = (len(types)/len(A))*100
  return TTR

def ttr(source_par, target_par):
  A = ttr_calc(source_par)
  B = ttr_calc(target_par)
  # types_A = nltk.Counter(A)
  # types_B = nltk.Counter(B)
  # TTR_A = (len(types_A)/len(A))*100
  # TTR_B = (len(types_B)/len(B))*100
  if A >= 80:
    print("The SOURCE paragraph is has a very rich vocabulary")
  elif A >= 60 and A < 80:
    print("The vocabulary richness of the SOURCE paragraph is about average")
  else:
    print("The SOURCE paragraph has a poor vocabulary richness")
  if B >= 80:
    print("The TARGET paragraph is has a very rich vocabulary")
  elif B >= 60 and B < 80:
    print("The vocabulary richness of the TARGET paragraph is about average")
  else:
    print("The TARGET paragraph has a poor vocabulary richness")

ttr(unseen_document, unseen_document3)