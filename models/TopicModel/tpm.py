# -*- coding: utf-8 -*-
"""TPM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dzr82AEnftc3CzW6s0hv0M6Q6OpxticJ
"""

import pickle
import gensim
from scipy.spatial import distance
dictionary = gensim.corpora.Dictionary.load('../models/TopicModel/dictionary.gensim')
corpus = pickle.load(open('../models/TopicModel/bow_corpus.pkl', 'rb'))
lda25 = gensim.models.ldamodel.LdaModel.load('../models/TopicModel/model25BOW.gensim')
from numpy import dot
from numpy.linalg import norm
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import pandas as pd

import nltk
nltk.download('wordnet')

stemmer = SnowballStemmer('english')

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

"""**SIMILARITY SCORE**"""

def topic_model(paragraph):
  bow_vector = dictionary.doc2bow(preprocess(paragraph))
  vector = lda25[bow_vector]
  return score_vectorizer(vector)

def score_vectorizer(a):
  # k = [(1, 0.2), (9, 0.3)]
  new_v = [None] * 25
  count = 0

  for i in range(len(new_v)):
      if i in [i[0] for i in a ]:
          # print(i)
          new_v[i] = a[count]
          count+=1
      else:
          new_v[i] = (i, 0.0)
      
          
  return dist_vec(new_v)

def dist_vec(a):
  t = []
  for i in a:
    t.append(i[1])
  return t

def sim(source_par, target_par):
  cos_sim = dot(topic_model(source_par), topic_model(target_par))/(norm(topic_model(source_par))*norm(topic_model(target_par)))
  return cos_sim

"""**EXPLANATIONS**"""

def list_converter(old_list):
  new_list =[]
  for i in old_list:
    new_list.append(list(i))
  return new_list

def label_topics(hu):
  for i in hu:
    if i[0] == 0:
      i[0] = "Neural Networks"
    if i[0] == 1:
      i[0] = "Matrix Decomposition"
    if i[0] == 2:
      i[0] = "Probabilistic Models"
    if i[0] == 3:
      i[0] = "Model Performance Evaluation"
    if i[0] == 4:
      i[0] = "Distance Measures"
    if i[0] == 5:
      i[0] = "Hyper-Paraneter Tuning"
    if i[0] == 6:
      i[0] = "Problem Statement / Research Topic"
    if i[0] == 7:
      i[0] = "Concept / Experiment Set-up"
    if i[0] == 8:
      i[0] = "Kernel / Loss Functions"
    if i[0] == 9:
      i[0] = "Objective Functions"
    if i[0] == 10:
      i[0] = "Topic Modelling"
    if i[0] == 11:
      i[0] = "Computational Complexity"
    if i[0] == 12:
      i[0] = "Image Processing"
    if i[0] == 13:
      i[0] = "Theorems and Proofs"
    if i[0] == 14:
      i[0] = "Game Theory"
    if i[0] == 15:
      i[0] = "Classification"
    if i[0] == 16:
      i[0] = "Computer Vision"
    if i[0] == 17:
      i[0] = "Clustering"
    if i[0] == 18:
      i[0] = "Exploratory Data Analysis"
    if i[0] == 19:
      i[0] = "Gradient Descent"
    if i[0] == 20:
      i[0] = "Academic Research (Paper Structure)"
    if i[0] == 21:
      i[0] = "Statistical Estimation"
    if i[0] == 22:
      i[0] = "Generative Models"
    if i[0] == 23:
      i[0] = "Reinforcement Learning"
    if i[0] == 24:
      i[0] = "Decision Trees / Ensemble Learning"
    
  return hu

def explanations(source_par, target_par):
  bow_vector1 = dictionary.doc2bow(preprocess(source_par))
  bow_vector2 = dictionary.doc2bow(preprocess(target_par))
  vec1 = lda25[bow_vector1]
  vec2 = lda25[bow_vector2]
  par1 = list_converter(vec1)
  par2 = list_converter(vec2)
  par1 = label_topics(par1)
  par2 = label_topics(par2)
  ans = []
  exp = ''
  for i in par1:
    for j in par2:
      if i[0] == j[0] and i[1] > 0.39 and j[1] > 0.39:
        ans.append("Likely to be Similar because they share topic \"" + str(i[0] + "\" in significant percentages"))
        break
      elif (i[0] == j[0] and i[1] > 0.39 and j[1] > 0.19 and j[1] < 0.39) or (i[0] == j[0] and j[1] > 0.39 and i[1] > 0.19 and j[1] < 0.39):
        ans.append("Might be similar as they share common topic \"" + i[0] + "\" albeit in different percentages ")
        break
      elif i[0] == j[0] and i[1] < 0.18 and j[1] < 0.18:
        ans.append("The paragraphs are likely to be dissimilar but they share small probabilities on \"" + str(i[0]) + "\" topic")
        break
      else:
        ans.append("Very likely to be dissimilar as they share no common topic")
  myset = set(ans)
  a = list(myset)
  sorted_list = sorted(a)
  for i in sorted_list:
    if i.startswith("Likely"):
      exp = i
      # print(i)
      break
    elif i.startswith("Might be similar"):
      exp = i
      # print(i)
      break
    elif i.startswith("The paragraphs"):
      exp = i
      # print(i)
      break
    elif i.startswith("Very likely"):
      exp = i
      # print(i)
      break
    # else:
    #   print("Very likely ")
    # break
  
  return print(exp)

"""**TEST**"""

unseen_document = "The recommendation problem can be defined as estimating the response of a user for new items, based on historical information stored in the system, \
and suggesting to this user novel and original items for which the predicted response is high. User item responses can be numerical values known as ratings (e.g., 1â€“5 stars),\
ordinal values (e.g., strongly agree, agree, neutral, disagree, strongly disagree) representing the possible levels of user appreciation, or binary values \
(e.g., like/dislike or interested/not interested). Moreover, user responses can be obtained explicitly, forinstance, through ratings/reviews entered by users in the system, or implicitly, from purchase history or access patterns [39, 70].\
For the purpose of simplicity, from this point on, we will call rating any type of user-item response."

unseen_document1 = 'An artificial neural network has a more complex structure than that of a\
perceptron model. The additional complexities may arise in a number of ways:\
1. The network may contain several intermediary layers between its input\
and output layers. Such intermediary layers are called hidden layers\
and the nodes embedded in these layers are called hidden nodes. The\
resulting structure is known as a multilayer neural network (see Figure\
5.17). In a feed-forward neural network, the nodes in one layer\
.Example of a multilayfeere d{onrvaarrdti ficianle uranl etwor(kA NN).\
are connected only to the nodes in the next layer. The perceptron is a\
single-layer, feed-forward neural network because it has only one layer\
of nodes-the output layer that performs complex mathematical operations.\
fn a recurrent neural network, the links may connect nodes\
within the same layer or nodes from one layer to the previous layers.\
2. The network may use types of activation functions other than the sign\
function. Examples of other activation functions include linear, sigmoid\
(logistic), and hyperbolic tangent functions, as shown in Figure 5.18.\
These activation functions allow the hidden and output nodes to produce\
output values that are nonlinear in their input parameters.'

unseen_document2 = 'There are different types of neural networks and the differences between them lies in their work principles, the scheme of actions, and the application areas. \
Convolutional neural networks (CNN) are mostly used for image recognition, and rarely for audio recognition. It is mostly applied to images because there is no need to check all the pixels one by one. \
CNN checks an image by blocks, starting from the left upper corner and moving further pixel by pixel up to a successful completion. Then the result of every verification is passed through a convolutional layer, \
where data elements have connections while others donâ€™t. Based on this data, the system can produce the result of the verifications and can conclude what is in the picture'

unseen_document3 = 'Generally speaking, the reason people could be interested in using a recommender system is that they have so many items to choose from \
in a limited period of timeâ€”that they cannot evaluate all the possible options. A recommender should be able to select and \
filter all this information to the user. Nowadays, the most successful recommender systems have been built for entertainment content domains, \
such as: movies, music, or books.'

a = explanations(unseen_document, unseen_document3)
a

b = sim(unseen_document, unseen_document3)
b